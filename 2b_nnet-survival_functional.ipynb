{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-28 13:17:35.943101: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-28 13:17:44.839513: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/biotools/python/3.8.1/lib:/usr/local/biotools/cuda/11.7/lib64:/usr/local/biotools/cuda/11.7/nccl/lib:\n",
      "2023-06-28 13:17:44.839610: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/biotools/python/3.8.1/lib:/usr/local/biotools/cuda/11.7/lib64:/usr/local/biotools/cuda/11.7/nccl/lib:\n",
      "2023-06-28 13:17:44.839617: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib as plt\n",
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "import tensorflow as tf\n",
    "\n",
    "import math\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.utils import concordance_index\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "\n",
    "from preprocessor.nnet_survival import nnet_survival\n",
    "from preprocessor.tabular import nnet_survival_trainer as st\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%writefile trainer/model.py\n",
    "import datetime\n",
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import tensorflow as tf\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Determine CSV, label, and key columns\n",
    "CSV_COLUMNS = [\n",
    "    \"patientid\",\n",
    "    \"task_1\",\n",
    "    \"task_2\",\n",
    "    \"centerid\",\n",
    "    \"age\",\n",
    "    \"weight\",\n",
    "    \"tobacco\",\n",
    "    \"alcohol\",\n",
    "    \"performance_status\",\n",
    "    \"hpv_status\",\n",
    "    \"surgery\",\n",
    "    \"chemotherapy\",\n",
    "    \"relapse\",\n",
    "    \"rfs\",    \n",
    "    \"gender_m\",\n",
    "]\n",
    "\n",
    "NUMERICAL_COLUMNS = [\"age\", \"weight\"]\n",
    "CATEGORICAL_COLUMNS = [\"centerid\", \"gender_m\", \"tobacco\", \"alcohol\",\n",
    "                    \"performance_status\", \"hpv_status\",\n",
    "                    \"surgery\", \"chemotherapy\", ]\n",
    "UNWANTED_COLS = [\"patientid\", \"task_1\", \"task_2\"]\n",
    "\n",
    "# Set default values for each CSV column.\n",
    "# Treat is_male and plurality as strings.\n",
    "DEFAULTS = [\n",
    "    \"null\",\n",
    "    [0],\n",
    "    [0],\n",
    "    \"null\",\n",
    "    [0.0],\n",
    "    [0.0],\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "]   \n",
    "\n",
    "del survival_model\n",
    "\n",
    "survival_model = st.TrainerNNetSurvival(\n",
    "    input_file_train = \"../data/task2/train_data.csv\",\n",
    "    input_file_eval = \"../data/task2/test_data.csv\",\n",
    "    output_dir = \"output\",\n",
    "    unwanted_cols = UNWANTED_COLS,\n",
    "    numerical_cols = NUMERICAL_COLUMNS,\n",
    "    categorical_cols =CATEGORICAL_COLUMNS,\n",
    "    train_batch_size = 32,\n",
    "    eval_batch_size = 100,\n",
    "    num_train_examples = 5000,\n",
    "    num_epochs = 500,\n",
    "    halflife= 1460)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Here is our Wide-and-Deep architecture so far:\n",
      "\n",
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " dense_9 (Dense)             (None, 64)                704       \n",
      "                                                                 \n",
      " dense_10 (Dense)            (None, 32)                2080      \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 32)                0         \n",
      "                                                                 \n",
      " dense_11 (Dense)            (None, 19)                627       \n",
      "                                                                 \n",
      " activation_3 (Activation)   (None, 19)                0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 3,411\n",
      "Trainable params: 3,411\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n",
      "Epoch 1/500\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m survival_model\u001b[39m.\u001b[39;49mtrain_and_evaluate()\n",
      "File \u001b[0;32m~/research/capstone/codebase/preprocessor/tabular/nnet_survival_trainer.py:306\u001b[0m, in \u001b[0;36mTrainerNNetSurvival.train_and_evaluate\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    295\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    296\u001b[0m \u001b[39mhistory = self.model.fit(\u001b[39;00m\n\u001b[1;32m    297\u001b[0m \u001b[39m    self.trainds,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    302\u001b[0m \u001b[39m    callbacks=[cp_callback])\u001b[39;00m\n\u001b[1;32m    303\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[1;32m    305\u001b[0m early_stopping \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mkeras\u001b[39m.\u001b[39mcallbacks\u001b[39m.\u001b[39mEarlyStopping(monitor\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mloss\u001b[39m\u001b[39m'\u001b[39m, patience\u001b[39m=\u001b[39m\u001b[39m50\u001b[39m)\n\u001b[0;32m--> 306\u001b[0m history\u001b[39m=\u001b[39m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mmodel\u001b[39m.\u001b[39;49mfit(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtrainds, \n\u001b[1;32m    307\u001b[0m                        validation_data \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mevalds, \n\u001b[1;32m    308\u001b[0m                        epochs\u001b[39m=\u001b[39;49m\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mnum_epochs, \n\u001b[1;32m    309\u001b[0m                        steps_per_epoch \u001b[39m=\u001b[39;49m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49msteps_per_epoch,\n\u001b[1;32m    310\u001b[0m                        verbose\u001b[39m=\u001b[39;49m\u001b[39m1\u001b[39;49m, \n\u001b[1;32m    311\u001b[0m                        callbacks\u001b[39m=\u001b[39;49m[early_stopping])\n",
      "File \u001b[0;32m~/research/venv/venv_2/lib/python3.8/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[39m=\u001b[39m _process_traceback_frames(e\u001b[39m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[39m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[39m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[39mraise\u001b[39;00m e\u001b[39m.\u001b[39mwith_traceback(filtered_tb) \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m     71\u001b[0m \u001b[39mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[39mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/research/venv/venv_2/lib/python3.8/site-packages/keras/engine/training.py:1662\u001b[0m, in \u001b[0;36mModel.fit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1660\u001b[0m logs \u001b[39m=\u001b[39m tf_utils\u001b[39m.\u001b[39msync_to_numpy_or_python_type(logs)\n\u001b[1;32m   1661\u001b[0m \u001b[39mif\u001b[39;00m logs \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n\u001b[0;32m-> 1662\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[1;32m   1663\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mUnexpected result of `train_function` \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1664\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m(Empty logs). Please use \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1665\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`Model.compile(..., run_eagerly=True)`, or \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1666\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m`tf.config.run_functions_eagerly(True)` for more \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1667\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39minformation of where went wrong, or file a \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1668\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39missue/bug to `tf.keras`.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[1;32m   1669\u001b[0m     )\n\u001b[1;32m   1670\u001b[0m \u001b[39m# Override with model metrics instead of last step logs\u001b[39;00m\n\u001b[1;32m   1671\u001b[0m logs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_validate_and_get_metrics_result(logs)\n",
      "\u001b[0;31mValueError\u001b[0m: Unexpected result of `train_function` (Empty logs). Please use `Model.compile(..., run_eagerly=True)`, or `tf.config.run_functions_eagerly(True)` for more information of where went wrong, or file a issue/bug to `tf.keras`."
     ]
    }
   ],
   "source": [
    "survival_model.train_and_evaluate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###################################################################################\n",
    "#Flexible model: Convolutional neural network using MNIST data\n",
    "#Uses some code from https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
    "#For this example, we use images of the handwritten numbers 0 through 4.\n",
    "#Larger numbers have shorter average survival. Task: Given an image of a number, predict survival curve.\n",
    "#Described in paper.\n",
    "\n",
    "\n",
    "# We need to split the data set into train and test, and fill in time & event for both groups\n",
    "train_data = pd.read_csv(\"../data/task2/train_data.csv\")\n",
    "test_data = pd.read_csv(\"../data/task2/test_data.csv\")\n",
    "time = train_data['rfs']\n",
    "event = train_data['relapse']\n",
    "timeTest = test_data['rfs']\n",
    "eventTest = test_data['relapse']\n",
    "x_train = train_data[['centerid',\n",
    "                      'age',\n",
    "                      'weight',\n",
    "                      'tobacco',\n",
    "                      'alcohol',\n",
    "                      'performance_status',\n",
    "                      'hpv_status',\n",
    "                      'surgery',\n",
    "                      'chemotherapy',\n",
    "                      'gender_m']].values\n",
    "x_test = test_data[['centerid',\n",
    "                      'age',\n",
    "                      'weight',\n",
    "                      'tobacco',\n",
    "                      'alcohol',\n",
    "                      'performance_status',\n",
    "                      'hpv_status',\n",
    "                      'surgery',\n",
    "                      'chemotherapy',\n",
    "                      'gender_m']].values\n",
    "\n",
    "#Convert event data to array format\n",
    "\n",
    "halflife=365.*4\n",
    "breaks=-np.log(1-np.arange(0.0,0.96,0.05))*halflife/np.log(2) \n",
    "#breaks=np.concatenate((np.arange(0,200,10),np.arange(200,1001,25)))\n",
    "\n",
    "n_intervals=len(breaks)-1\n",
    "timegap = breaks[1:] - breaks[:-1]\n",
    "y_train_array=nnet_survival.make_surv_array(time,event,breaks)\n",
    "\n",
    "#Train model\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(10,),activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "prop_hazards=0\n",
    "if prop_hazards:\n",
    "\tmodel.add(Dense(1, use_bias=0, kernel_initializer='zeros'))\n",
    "\tmodel.add(nnet_survival.PropHazards(n_intervals))\n",
    "else:\n",
    "\tmodel.add(Dense(n_intervals, kernel_initializer='zeros', bias_initializer='zeros'))\n",
    "\tmodel.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss=nnet_survival.surv_likelihood(n_intervals), optimizer=tf.keras.optimizers.Adam())\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=50)\n",
    "history=model.fit(x_train, y_train_array, batch_size=64, epochs=10000, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "#Training set results\n",
    "y_pred=model.predict(x_train,verbose=0)\n",
    "\n",
    "\n",
    "#discrimination (C-index)\n",
    "oneyr_surv=np.cumprod(y_pred[:,0:np.nonzero(breaks>365)[0][0]], axis=1)[:,-1]\n",
    "print(concordance_index(time,oneyr_surv,event))\n",
    "\n",
    "#calibration plot\n",
    "days_plot = 365*4\n",
    "plt.subplot(1,1,1)\n",
    "#plt.subplot(1, 2, 1)\n",
    "kmf = KaplanMeierFitter()\n",
    "matplotlib.style.use('default')\n",
    "actual = []\n",
    "predicted = []\n",
    "kmf.fit(time, event_observed=event)\n",
    "actual.append(plt.plot(kmf.survival_function_.index.values, kmf.survival_function_.KM_estimate,ls='--',c='r'))\n",
    "pred_surv=np.mean(np.cumprod(y_pred, axis=1),axis=0)\n",
    "predicted.append(plt.plot(breaks,np.concatenate(([1],pred_surv)),ls='-',c='b'))\n",
    "#print(i, kmf.median_)\n",
    "\n",
    "plt.xticks(np.arange(0, days_plot+0.0001, 200))\n",
    "plt.yticks(np.arange(0, 1.0001, 0.125))\n",
    "plt.xlim([0,days_plot])\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel('Follow-up time (days)')\n",
    "plt.ylabel('Proportion surviving')\n",
    "plt.title('Training set calibration')\n",
    "#plt.show()\n",
    "\n",
    "#Test set results\n",
    "y_pred=model.predict(x_test,verbose=0)\n",
    "\n",
    "#discrimination (C-index)\n",
    "oneyr_surv=np.cumprod(y_pred[:,0:np.nonzero(breaks>365)[0][0]], axis=1)[:,-1]\n",
    "print(concordance_index(timeTest,oneyr_surv,eventTest))\n",
    "\n",
    "#discrimination of perfect model that uses actual digit as survival time predictor\n",
    "#print(concordance_index(timeTest,-y_test.astype('float'),eventTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_2",
   "language": "python",
   "name": "venv_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
