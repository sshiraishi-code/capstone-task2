{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-25 17:50:20.515230: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-25 17:50:21.863668: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/biotools/python/3.8.1/lib:/usr/local/biotools/cuda/11.7/lib64:/usr/local/biotools/cuda/11.7/nccl/lib:\n",
      "2023-06-25 17:50:21.863756: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /usr/local/biotools/python/3.8.1/lib:/usr/local/biotools/cuda/11.7/lib64:/usr/local/biotools/cuda/11.7/nccl/lib:\n",
      "2023-06-25 17:50:21.863762: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "from tensorflow.keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Sequential, Model\n",
    "from tensorflow.keras.layers import Input, Dense, Dropout, Activation, LSTM, GRU, Embedding, Concatenate, Conv1D, GlobalMaxPooling1D, MaxPooling1D, GlobalAveragePooling1D, BatchNormalization, TimeDistributed\n",
    "import tensorflow.keras.backend as K\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.models import load_model\n",
    "import tensorflow.keras\n",
    "\n",
    "import math\n",
    "from lifelines import KaplanMeierFitter\n",
    "from lifelines import CoxPHFitter\n",
    "from lifelines.utils import concordance_index\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy import stats\n",
    "\n",
    "from preprocessor.nnet_survival import nnet_survival\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 45\u001b[0m\n\u001b[1;32m     40\u001b[0m     label \u001b[39m=\u001b[39m row_data\u001b[39m.\u001b[39mpop(LABEL_COLUMN)\n\u001b[1;32m     42\u001b[0m     \u001b[39mreturn\u001b[39;00m row_data, label  \u001b[39m# features, label\u001b[39;00m\n\u001b[0;32m---> 45\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mload_dataset\u001b[39m(pattern, batch_size\u001b[39m=\u001b[39m\u001b[39m1\u001b[39m, mode\u001b[39m=\u001b[39mtf\u001b[39m.\u001b[39mestimator\u001b[39m.\u001b[39mModeKeys\u001b[39m.\u001b[39mEVAL):\n\u001b[1;32m     46\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"Loads dataset using the tf.data API from CSV files.\u001b[39;00m\n\u001b[1;32m     47\u001b[0m \n\u001b[1;32m     48\u001b[0m \u001b[39m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[39m        `Dataset` object.\u001b[39;00m\n\u001b[1;32m     54\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n\u001b[1;32m     55\u001b[0m     \u001b[39m# Make a CSV dataset\u001b[39;00m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# Determine CSV, label, and key columns\n",
    "CSV_COLUMNS = [\n",
    "    \"patientid\",\n",
    "    \"task_1\",\n",
    "    \"task_2\",\n",
    "    \"centerid\",\n",
    "    \"gender\",\n",
    "    \"age\",\n",
    "    \"weight\",\n",
    "    \"tobacco\",\n",
    "    \"alcohol\",\n",
    "    \"performance_status\",\n",
    "    \"hpv_status\",\n",
    "    \"surgery\",\n",
    "    \"chemotherapy\",\n",
    "    \"relapse\",\n",
    "    \"rfs\",    \n",
    "]\n",
    "LABEL_COLUMN = \"weight_pounds\"\n",
    "\n",
    "NUMERICAL_COLUMNS = [\"age\", \"weight\"]\n",
    "CATEGORICAL_COLUMNS = [\"centerid\", \"gender\", \"tobacco\", \"alcohol\",\n",
    "                       \"performance_status\", \"hpv_status\",\n",
    "                       \"surgery\", \"chemotherapy\", ]\n",
    "UNWANTED_COLS = [\"patientid\", \"task_1\", \"task_2\"]\n",
    "\n",
    "# Set default values for each CSV column.\n",
    "# Treat is_male and plurality as strings.\n",
    "DEFAULTS = [[0.0], [\"null\"], [0.0], [\"null\"], [0.0]]\n",
    "\n",
    "\n",
    "def features_and_labels(row_data):\n",
    "    \"\"\"Splits features and labels from feature dictionary.\n",
    "\n",
    "    Args:\n",
    "        row_data: Dictionary of CSV column names and tensor values.\n",
    "    Returns:\n",
    "        Dictionary of feature tensors and label tensor.\n",
    "    \"\"\"\n",
    "    label = row_data.pop(LABEL_COLUMN)\n",
    "\n",
    "    return row_data, label  # features, label\n",
    "\n",
    "\n",
    "def load_dataset(pattern, batch_size=1, mode=tf.estimator.ModeKeys.EVAL):\n",
    "    \"\"\"Loads dataset using the tf.data API from CSV files.\n",
    "\n",
    "    Args:\n",
    "        pattern: str, file pattern to glob into list of files.\n",
    "        batch_size: int, the number of examples per batch.\n",
    "        mode: tf.estimator.ModeKeys to determine if training or evaluating.\n",
    "    Returns:\n",
    "        `Dataset` object.\n",
    "    \"\"\"\n",
    "    # Make a CSV dataset\n",
    "    dataset = tf.data.experimental.make_csv_dataset(\n",
    "        file_pattern=pattern,\n",
    "        batch_size=batch_size,\n",
    "        column_names=CSV_COLUMNS,\n",
    "        column_defaults=DEFAULTS,\n",
    "    )\n",
    "\n",
    "    # Map dataset to features and label\n",
    "    dataset = dataset.map(map_func=features_and_labels)  # features, label\n",
    "\n",
    "    # Shuffle and repeat for training\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        dataset = dataset.shuffle(buffer_size=1000).repeat()\n",
    "\n",
    "    # Take advantage of multi-threading; 1=AUTOTUNE\n",
    "    dataset = dataset.prefetch(buffer_size=1)\n",
    "\n",
    "    return dataset\n",
    "\n",
    "\n",
    "def create_input_layers():\n",
    "    \"\"\"Creates dictionary of input layers for each feature.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of `tf.Keras.layers.Input` layers for each feature.\n",
    "    \"\"\"\n",
    "    deep_inputs = {\n",
    "        colname: tf.keras.layers.Input(\n",
    "            name=colname, shape=(1,), dtype=\"float32\"\n",
    "        )\n",
    "        for colname in NUMERICAL_COLUMNS\n",
    "    }\n",
    "\n",
    "    wide_inputs = {\n",
    "        colname: tf.keras.layers.Input(name=colname, shape=(1,), dtype=\"string\")\n",
    "        for colname in CATEGORICAL_COLUMNS\n",
    "    }\n",
    "\n",
    "    inputs = {**wide_inputs, **deep_inputs}\n",
    "\n",
    "    return inputs\n",
    "\n",
    "\n",
    "def transform(inputs, nembeds):\n",
    "    \"\"\"Creates dictionary of transformed inputs.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of transformed Tensors\n",
    "    \"\"\"\n",
    "\n",
    "    deep = {}\n",
    "    wide = {}\n",
    "\n",
    "    buckets = {\n",
    "        \"mother_age\": np.arange(15, 45, 1).tolist(),\n",
    "        \"gestation_weeks\": np.arange(17, 47, 1).tolist(),\n",
    "    }\n",
    "    bucketized = {}\n",
    "\n",
    "    for numerical_column in NUMERICAL_COLUMNS:\n",
    "        deep[numerical_column] = inputs[numerical_column]\n",
    "        bucketized[numerical_column] = tf.keras.layers.Discretization(buckets[numerical_column])(inputs[numerical_column])\n",
    "        wide[f\"btk_{numerical_column}\"] = tf.keras.layers.CategoryEncoding(\n",
    "            num_tokens=len(buckets[numerical_column]) + 1, output_mode=\"one_hot\"\n",
    "        )(bucketized[numerical_column])\n",
    "\n",
    "    crossed = tf.keras.layers.experimental.preprocessing.HashedCrossing(\n",
    "        num_bins=len(buckets[\"mother_age\"]) * len(buckets[\"gestation_weeks\"])\n",
    "    )((bucketized[\"mother_age\"], bucketized[\"gestation_weeks\"]))\n",
    "\n",
    "    deep[\"age_gestation_embeds\"] = tf.keras.layers.Flatten()(\n",
    "        tf.keras.layers.Embedding(\n",
    "            input_dim=len(buckets[\"mother_age\"])\n",
    "            * len(buckets[\"gestation_weeks\"]),\n",
    "            output_dim=nembeds,\n",
    "        )(crossed)\n",
    "    )\n",
    "\n",
    "    vocab = {\n",
    "        \"is_male\": [\"True\", \"False\", \"Unknown\"],\n",
    "        \"plurality\": [\n",
    "            \"Single(1)\",\n",
    "            \"Twins(2)\",\n",
    "            \"Triplets(3)\",\n",
    "            \"Quadruplets(4)\",\n",
    "            \"Quintuplets(5)\",\n",
    "            \"Multiple(2+)\",\n",
    "        ],\n",
    "    }\n",
    "\n",
    "    for categorical_column in CATEGORICAL_COLUMNS:\n",
    "        wide[categorical_column] = tf.keras.layers.StringLookup(\n",
    "            vocabulary=vocab[categorical_column], output_mode=\"one_hot\"\n",
    "        )(inputs[categorical_column])\n",
    "\n",
    "    return wide, deep\n",
    "\n",
    "def get_model_outputs(wide_inputs, deep_inputs, dnn_hidden_units):\n",
    "    \"\"\"Creates model architecture and returns outputs.\n",
    "\n",
    "    Args:\n",
    "        wide_inputs: Dense tensor used as inputs to wide side of model.\n",
    "        deep_inputs: Dense tensor used as inputs to deep side of model.\n",
    "        dnn_hidden_units: List of integers where length is number of hidden\n",
    "            layers and ith element is the number of neurons at ith layer.\n",
    "    Returns:\n",
    "        Dense tensor output from the model.\n",
    "    \"\"\"\n",
    "    # Hidden layers for the deep side\n",
    "    layers = [int(x) for x in dnn_hidden_units.split()]\n",
    "    deep = deep_inputs\n",
    "    for layerno, numnodes in enumerate(layers):\n",
    "        deep = tf.keras.layers.Dense(\n",
    "            units=numnodes, activation=\"relu\", name=f\"dnn_{layerno + 1}\"\n",
    "        )(deep)\n",
    "    deep_out = deep\n",
    "\n",
    "    # Linear model for the wide side\n",
    "    wide_out = tf.keras.layers.Dense(\n",
    "        units=10, activation=\"relu\", name=\"linear\"\n",
    "    )(wide_inputs)\n",
    "\n",
    "    # Concatenate the two sides\n",
    "    both = tf.keras.layers.Concatenate(name=\"both\")([deep_out, wide_out])\n",
    "\n",
    "    # Final output is a linear activation because this is regression\n",
    "    output = tf.keras.layers.Dense(units=1, activation=\"linear\", name=\"weight\")(\n",
    "        both\n",
    "    )\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "def rmse(y_true, y_pred):\n",
    "    \"\"\"Calculates RMSE evaluation metric.\n",
    "\n",
    "    Args:\n",
    "        y_true: tensor, true labels.\n",
    "        y_pred: tensor, predicted labels.\n",
    "    Returns:\n",
    "        Tensor with value of RMSE between true and predicted labels.\n",
    "    \"\"\"\n",
    "    return tf.sqrt(tf.reduce_mean(tf.square(y_pred - y_true)))\n",
    "\n",
    "\n",
    "def build_wide_deep_model(dnn_hidden_units=[64, 32], nembeds=3):\n",
    "    \"\"\"Builds wide and deep model using Keras Functional API.\n",
    "\n",
    "    Returns:\n",
    "        `tf.keras.models.Model` object.\n",
    "    \"\"\"\n",
    "    # Create input layers\n",
    "    inputs = create_input_layers()\n",
    "\n",
    "    # transform raw features for both wide and deep\n",
    "    wide, deep = transform(inputs, nembeds)\n",
    "\n",
    "    # The Functional API in Keras requires: LayerConstructor()(inputs)\n",
    "    wide_inputs = tf.keras.layers.Concatenate()(wide.values())\n",
    "    deep_inputs = tf.keras.layers.Concatenate()(deep.values())\n",
    "\n",
    "    # Get output of model given inputs\n",
    "    output = get_model_outputs(wide_inputs, deep_inputs, dnn_hidden_units)\n",
    "\n",
    "    # Build model and compile it all together\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=output)\n",
    "    model.compile(optimizer=\"adam\", loss=\"mse\", metrics=[rmse, \"mse\"])\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# Instantiate the HyperTune reporting object\n",
    "hpt = hypertune.HyperTune()\n",
    "\n",
    "# Reporting callback\n",
    "class HPTCallback(tf.keras.callbacks.Callback):\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        global hpt\n",
    "        hpt.report_hyperparameter_tuning_metric(\n",
    "            hyperparameter_metric_tag='val_rmse',\n",
    "            metric_value=logs['val_rmse'],\n",
    "            global_step=epoch)\n",
    "\n",
    "        \n",
    "def train_and_evaluate(args):\n",
    "    model = build_wide_deep_model(args[\"nnsize\"], args[\"nembeds\"])\n",
    "    print(\"Here is our Wide-and-Deep architecture so far:\\n\")\n",
    "    print(model.summary())\n",
    "\n",
    "    trainds = load_dataset(\n",
    "        args[\"train_data_path\"],\n",
    "        args[\"batch_size\"],\n",
    "        tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "    evalds = load_dataset(\n",
    "        args[\"eval_data_path\"], 1000, tf.estimator.ModeKeys.EVAL)\n",
    "    if args[\"eval_steps\"]:\n",
    "        evalds = evalds.take(count=args[\"eval_steps\"])\n",
    "\n",
    "    num_batches = args[\"batch_size\"] * args[\"num_epochs\"]\n",
    "    steps_per_epoch = args[\"train_examples\"] // num_batches\n",
    "\n",
    "    checkpoint_path = os.path.join(args[\"output_dir\"], \"checkpoints/babyweight\")\n",
    "    cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "        filepath=checkpoint_path, verbose=1, save_weights_only=True)\n",
    "\n",
    "    history = model.fit(\n",
    "        trainds,\n",
    "        validation_data=evalds,\n",
    "        epochs=args[\"num_epochs\"],\n",
    "        steps_per_epoch=steps_per_epoch,\n",
    "        verbose=2,  # 0=silent, 1=progress bar, 2=one line per epoch\n",
    "        callbacks=[cp_callback, HPTCallback()])\n",
    "\n",
    "    EXPORT_PATH = os.path.join(\n",
    "        args[\"output_dir\"], datetime.datetime.now().strftime(\"%Y%m%d%H%M%S\"))\n",
    "    tf.saved_model.save(\n",
    "        obj=model, export_dir=EXPORT_PATH)  # with default serving function\n",
    "    \n",
    "    print(\"Exported trained model to {}\".format(EXPORT_PATH))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "###################################################################################\n",
    "#Flexible model: Convolutional neural network using MNIST data\n",
    "#Uses some code from https://github.com/keras-team/keras/blob/master/examples/mnist_cnn.py\n",
    "#For this example, we use images of the handwritten numbers 0 through 4.\n",
    "#Larger numbers have shorter average survival. Task: Given an image of a number, predict survival curve.\n",
    "#Described in paper.\n",
    "\n",
    "\n",
    "# We need to split the data set into train and test, and fill in time & event for both groups\n",
    "train_data = pd.read_csv(\"../data/task2/train_data.csv\")\n",
    "test_data = pd.read_csv(\"../data/task2/test_data.csv\")\n",
    "time = train_data['rfs']\n",
    "event = train_data['relapse']\n",
    "timeTest = test_data['rfs']\n",
    "eventTest = test_data['relapse']\n",
    "x_train = train_data[['centerid',\n",
    "                      'age',\n",
    "                      'weight',\n",
    "                      'tobacco',\n",
    "                      'alcohol',\n",
    "                      'performance_status',\n",
    "                      'hpv_status',\n",
    "                      'surgery',\n",
    "                      'chemotherapy',\n",
    "                      'gender_m']].values\n",
    "x_test = test_data[['centerid',\n",
    "                      'age',\n",
    "                      'weight',\n",
    "                      'tobacco',\n",
    "                      'alcohol',\n",
    "                      'performance_status',\n",
    "                      'hpv_status',\n",
    "                      'surgery',\n",
    "                      'chemotherapy',\n",
    "                      'gender_m']].values\n",
    "\n",
    "#Convert event data to array format\n",
    "\n",
    "halflife=365.*4\n",
    "breaks=-np.log(1-np.arange(0.0,0.96,0.05))*halflife/np.log(2) \n",
    "#breaks=np.concatenate((np.arange(0,200,10),np.arange(200,1001,25)))\n",
    "\n",
    "n_intervals=len(breaks)-1\n",
    "timegap = breaks[1:] - breaks[:-1]\n",
    "y_train_array=nnet_survival.make_surv_array(time,event,breaks)\n",
    "\n",
    "#Train model\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "import tensorflow as tf\n",
    "tf.random.set_seed(1)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Dense(64, input_shape=(10,),activation='relu'))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(Dropout(0.25))\n",
    "\n",
    "prop_hazards=0\n",
    "if prop_hazards:\n",
    "\tmodel.add(Dense(1, use_bias=0, kernel_initializer='zeros'))\n",
    "\tmodel.add(nnet_survival.PropHazards(n_intervals))\n",
    "else:\n",
    "\tmodel.add(Dense(n_intervals, kernel_initializer='zeros', bias_initializer='zeros'))\n",
    "\tmodel.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss=nnet_survival.surv_likelihood(n_intervals), optimizer=tf.keras.optimizers.Adam())\n",
    "early_stopping = EarlyStopping(monitor='loss', patience=50)\n",
    "history=model.fit(x_train, y_train_array, batch_size=64, epochs=10000, verbose=1, callbacks=[early_stopping])\n",
    "\n",
    "#Training set results\n",
    "y_pred=model.predict(x_train,verbose=0)\n",
    "\n",
    "\n",
    "#discrimination (C-index)\n",
    "oneyr_surv=np.cumprod(y_pred[:,0:np.nonzero(breaks>365)[0][0]], axis=1)[:,-1]\n",
    "print(concordance_index(time,oneyr_surv,event))\n",
    "\n",
    "#calibration plot\n",
    "days_plot = 365*4\n",
    "plt.subplot(1,1,1)\n",
    "#plt.subplot(1, 2, 1)\n",
    "kmf = KaplanMeierFitter()\n",
    "matplotlib.style.use('default')\n",
    "actual = []\n",
    "predicted = []\n",
    "kmf.fit(time, event_observed=event)\n",
    "actual.append(plt.plot(kmf.survival_function_.index.values, kmf.survival_function_.KM_estimate,ls='--',c='r'))\n",
    "pred_surv=np.mean(np.cumprod(y_pred, axis=1),axis=0)\n",
    "predicted.append(plt.plot(breaks,np.concatenate(([1],pred_surv)),ls='-',c='b'))\n",
    "#print(i, kmf.median_)\n",
    "\n",
    "plt.xticks(np.arange(0, days_plot+0.0001, 200))\n",
    "plt.yticks(np.arange(0, 1.0001, 0.125))\n",
    "plt.xlim([0,days_plot])\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel('Follow-up time (days)')\n",
    "plt.ylabel('Proportion surviving')\n",
    "plt.title('Training set calibration')\n",
    "#plt.show()\n",
    "\n",
    "#Test set results\n",
    "y_pred=model.predict(x_test,verbose=0)\n",
    "\n",
    "#discrimination (C-index)\n",
    "oneyr_surv=np.cumprod(y_pred[:,0:np.nonzero(breaks>365)[0][0]], axis=1)[:,-1]\n",
    "print(concordance_index(timeTest,oneyr_surv,eventTest))\n",
    "\n",
    "#discrimination of perfect model that uses actual digit as survival time predictor\n",
    "#print(concordance_index(timeTest,-y_test.astype('float'),eventTest))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "#calibration\n",
    "\"\"\" plt.subplot(1, 2, 2)\n",
    "kmf = KaplanMeierFitter()\n",
    "matplotlib.style.use('default')\n",
    "actual = []\n",
    "predicted = []\n",
    "for i in range(num_classes):\n",
    "\tkmf.fit(timeTest[y_test==i], event_observed=eventTest[y_test==i])\n",
    "\tactual.append(plt.plot(kmf.survival_function_.index.values, kmf.survival_function_.KM_estimate,ls='--',c='C'+str(i)))\n",
    "\tpred_surv=np.mean(np.cumprod(y_pred[y_test==i,:], axis=1),axis=0)\n",
    "\tpredicted.append(plt.plot(breaks,np.concatenate(([1],pred_surv)),ls='-',c='C'+str(i)))\n",
    "\n",
    "plt.xticks(np.arange(0, days_plot+0.0001, 200))\n",
    "plt.yticks(np.arange(0, 1.0001, 0.125))\n",
    "plt.xlim([0,days_plot])\n",
    "plt.ylim([0,1])\n",
    "plt.xlabel('Follow-up time (days)')\n",
    "plt.ylabel('Proportion surviving')\n",
    "plt.legend(['0: Actual','0: Predicted',\n",
    "\t'1: Actual','1: Predicted',\n",
    "\t'2: Actual','2: Predicted',\n",
    "\t'3: Actual','3: Predicted',\n",
    "\t'4: Actual','4: Predicted'])\n",
    "plt.title('Test set calibration')\n",
    "plt.show() \"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_2",
   "language": "python",
   "name": "venv_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
