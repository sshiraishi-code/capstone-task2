{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import datetime\n",
    "import os\n",
    "from copy import deepcopy\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from sklearn import model_selection\n",
    "\n",
    "from preprocessor.tabular import utils\n",
    "from preprocessor.nnet_survival import nnet_survival\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Determine CSV, label, and key columns\n",
    "CSV_COLUMNS = [\n",
    "    \"patientid\",\n",
    "    \"task_1\",\n",
    "    \"task_2\",\n",
    "    \"centerid\",\n",
    "    \"age\",\n",
    "    \"weight\",\n",
    "    \"tobacco\",\n",
    "    \"alcohol\",\n",
    "    \"performance_status\",\n",
    "    \"hpv_status\",\n",
    "    \"surgery\",\n",
    "    \"chemotherapy\",\n",
    "    \"relapse\",\n",
    "    \"rfs\",    \n",
    "    \"gender_m\",\n",
    "]\n",
    "\n",
    "NUMERICAL_COLUMNS = [\"age\", \"weight\"]\n",
    "CATEGORICAL_COLUMNS = [\"centerid\", \"gender_m\", \"tobacco\", \"alcohol\",\n",
    "                    \"performance_status\", \"hpv_status\",\n",
    "                    \"surgery\", \"chemotherapy\", ]\n",
    "UNWANTED_COLS = [\"patientid\", \"task_1\", \"task_2\"]\n",
    "\n",
    "# Set default values for each CSV column.\n",
    "# Treat is_male and plurality as strings.\n",
    "DEFAULTS = [\n",
    "    \"null\",\n",
    "    [0],\n",
    "    [0],\n",
    "    \"null\",\n",
    "    [0.0],\n",
    "    [0.0],\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "    [0],\n",
    "]   \n",
    "HALFLIFE = 1640.\n",
    "BREAKS =-np.log(1-np.arange(0.0,0.96,0.05))*HALFLIFE/np.log(2) \n",
    "\n",
    "\n",
    "def load_dataset(input_file, batch_size = 1, \n",
    "                mode = tf.estimator.ModeKeys.EVAL):\n",
    "    \"\"\"Loads dataset using the tf.data API from CSV files.\n",
    "    Args:\n",
    "        pattern: str, file pattern to glob into list of files.\n",
    "        batch_size: int, the number of examples per batch.\n",
    "        mode: tf.estimator.ModeKeys to determine if training or evaluating.\n",
    "    Returns:\n",
    "        `Dataset` object.\n",
    "    \"\"\"        \n",
    "    if not os.path.exists(input_file):\n",
    "        raise FileExistsError(f'{input_file} cannot be found.')\n",
    "    suffix = Path(input_file).suffix\n",
    "    if suffix not in ['.csv', '.xlsx', '.xls']:\n",
    "        raise ValueError(f'{suffix} type file not supported yet.')\n",
    "\n",
    "    raw_data = pd.DataFrame()\n",
    "    if suffix == '.csv':\n",
    "        raw_data = pd.read_csv(input_file, header=0)\n",
    "    elif suffix == '.xlsx':\n",
    "        raw_data = pd.read_excel(input_file, sheet_name=0, header=0, index_col=0)\n",
    "\n",
    "    # clean column names\n",
    "    raw_data.columns = raw_data.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "    # clean string values\n",
    "    df_obj = raw_data.select_dtypes(['object'])\n",
    "    raw_data[df_obj.columns] = df_obj.apply(\n",
    "        lambda x: x.str.strip().str.lower().str.replace(' ', '_'))\n",
    "    \n",
    "    # data type\n",
    "    for c in NUMERICAL_COLUMNS:\n",
    "        raw_data[c] = raw_data[c].astype('float32')\n",
    "    for c in CATEGORICAL_COLUMNS:\n",
    "        raw_data[c] = raw_data[c].astype('str')\n",
    "    \n",
    "    # calculate the training label for nnet survival\n",
    "    time = raw_data.pop('rfs')\n",
    "    event = raw_data.pop('relapse')\n",
    "    label=nnet_survival.make_surv_array(time,event,BREAKS)\n",
    "\n",
    "    for c in UNWANTED_COLS:\n",
    "        raw_data.pop(c)\n",
    "    \n",
    "    dataset = tf.data.Dataset.from_tensor_slices((dict(raw_data), label))\n",
    "    \n",
    "    # Shuffle and repeat for training\n",
    "    if mode == tf.estimator.ModeKeys.TRAIN:\n",
    "        dataset = dataset.shuffle(buffer_size=1000).repeat()\n",
    "\n",
    "    # Take advantage of multi-threading; 1=AUTOTUNE\n",
    "    dataset = dataset.prefetch(buffer_size=1)\n",
    "    \n",
    "    return dataset\n",
    "\n",
    "def create_input_layers():\n",
    "    \"\"\"Creates dictionary of input layers for each feature.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of `tf.Keras.layers.Input` layers for each feature.\n",
    "    \"\"\"\n",
    "    deep_inputs = {\n",
    "        colname: tf.keras.layers.Input(\n",
    "            name=colname, shape=(1,), dtype=\"float32\"\n",
    "        )\n",
    "        for colname in NUMERICAL_COLUMNS\n",
    "    }\n",
    "\n",
    "    wide_inputs = {\n",
    "        colname: tf.keras.layers.Input(name=colname, shape=(1,), dtype=\"string\")\n",
    "        for colname in CATEGORICAL_COLUMNS\n",
    "    }\n",
    "\n",
    "    inputs = {**wide_inputs, **deep_inputs}\n",
    "    print(inputs)\n",
    "    return inputs\n",
    "\n",
    "def transform(inputs, nembeds):\n",
    "    \"\"\"Creates dictionary of transformed inputs.\n",
    "\n",
    "    Returns:\n",
    "        Dictionary of transformed Tensors\n",
    "    \"\"\"\n",
    "\n",
    "    deep = {}\n",
    "    wide = {}\n",
    "    \n",
    "    buckets = {\n",
    "        \"age\": np.arange(30, 90, 12).tolist(),\n",
    "        \"weight\": np.arange(50, 160, 11).tolist(),\n",
    "    }\n",
    "    bucketized = {}\n",
    "\n",
    "    for numerical_column in NUMERICAL_COLUMNS:\n",
    "        deep[numerical_column] = inputs[numerical_column]\n",
    "        bucketized[numerical_column] = tf.keras.layers.Discretization(buckets[numerical_column])(inputs[numerical_column])\n",
    "        wide[f\"btk_{numerical_column}\"] = tf.keras.layers.CategoryEncoding(\n",
    "            num_tokens=len(buckets[numerical_column]) + 1, output_mode=\"one_hot\"\n",
    "        )(bucketized[numerical_column])\n",
    "\n",
    "    crossed = tf.keras.layers.experimental.preprocessing.HashedCrossing(\n",
    "        num_bins=len(buckets[\"age\"]) * len(buckets[\"weight\"])\n",
    "    )((bucketized[\"age\"], bucketized[\"weight\"]))\n",
    "\n",
    "    deep[\"age_weight_embeds\"] = tf.keras.layers.Flatten()(\n",
    "        tf.keras.layers.Embedding(\n",
    "            input_dim=len(buckets[\"age\"])\n",
    "            * len(buckets[\"weight\"]),\n",
    "            output_dim=nembeds,\n",
    "        )(crossed)\n",
    "    )\n",
    "\n",
    "    vocab = {\n",
    "        \"centerid\": [\"True\", \"False\", \"Unknown\"],\n",
    "        \"gender_m\": [\"0\", \"1\"],\n",
    "        \"tobacco\": [\"0\", \"1\", \"-1\"],\n",
    "        \"alcohol\": [\"0\", \"1\", \"-1\"],\n",
    "        \"performance_status\": [\"0\", \"1\", \"2\", \"3\", \"4\", \"-1\"],\n",
    "        \"hpv_status\": [\"0\", \"1\", \"-1\"],\n",
    "        \"surgery\": [\"0\", \"1\", \"-1\"],\n",
    "        \"chemotherapy\": [\"0\", \"1\"],\n",
    "    }\n",
    "\n",
    "    for categorical_column in CATEGORICAL_COLUMNS:\n",
    "        wide[categorical_column] = tf.keras.layers.StringLookup(\n",
    "            vocabulary=vocab[categorical_column], output_mode=\"one_hot\"\n",
    "        )(inputs[categorical_column])\n",
    "\n",
    "    return wide, deep\n",
    "\n",
    "def get_model_outputs(wide_inputs, deep_inputs, dnn_hidden_units: str = \"64 32\"):\n",
    "    \"\"\"Creates model architecture and returns outputs.\n",
    "\n",
    "    Args:\n",
    "        wide_inputs: Dense tensor used as inputs to wide side of model.\n",
    "        deep_inputs: Dense tensor used as inputs to deep side of model.\n",
    "        dnn_hidden_units: List of integers where length is number of hidden\n",
    "            layers and ith element is the number of neurons at ith layer.\n",
    "    Returns:\n",
    "        Dense tensor output from the model.\n",
    "    \"\"\"\n",
    "    # Hidden layers for the deep side\n",
    "    layers = [int(x) for x in dnn_hidden_units.split()]\n",
    "    deep = deep_inputs\n",
    "    for layerno, numnodes in enumerate(layers):\n",
    "        deep = tf.keras.layers.Dense(\n",
    "            units=numnodes, activation=\"relu\", name=f\"dnn_{layerno + 1}\"\n",
    "        )(deep)\n",
    "    deep_out = deep\n",
    "\n",
    "    # Linear model for the wide side\n",
    "    wide_out = tf.keras.layers.Dense(\n",
    "        units=10, activation=\"relu\", name=\"linear\"\n",
    "    )(wide_inputs)\n",
    "\n",
    "    # Concatenate the two sides\n",
    "    both = tf.keras.layers.Concatenate(name=\"both\")([deep_out, wide_out])\n",
    "\n",
    "    output1 = tf.keras.layers.Dense(\n",
    "        units=64, activation=\"relu\", name=\"dense\")(both)\n",
    "    \n",
    "    output2 = tf.keras.layers.Dropout(0.2)(output1)\n",
    "    \n",
    "    n_intervals = len(BREAKS)-1\n",
    "    # Final output is a linear activation because this is regression\n",
    "    output3 = tf.keras.layers.Dense(\n",
    "        units=n_intervals, activation=\"sigmoid\", name=\"haz\",\n",
    "        kernel_initializer='zeros', bias_initializer='zeros')(output2)\n",
    "\n",
    "    return output3\n",
    "\n",
    "def build_wide_deep_model(dnn_hidden_units=\"64 32\", nembeds=3):\n",
    "    \"\"\"Builds wide and deep model using Keras Functional API.\n",
    "\n",
    "    Returns:\n",
    "        `tf.keras.models.Model` object.\n",
    "    \"\"\"\n",
    "    # Create input layers\n",
    "    inputs = create_input_layers()\n",
    "\n",
    "    # transform raw features for both wide and deep\n",
    "    wide, deep = transform(inputs, nembeds)\n",
    "\n",
    "    # The Functional API in Keras requires: LayerConstructor()(inputs)\n",
    "    wide_inputs = tf.keras.layers.Concatenate()(wide.values())\n",
    "    deep_inputs = tf.keras.layers.Concatenate()(deep.values())\n",
    "\n",
    "    # Get output of model given inputs\n",
    "    output = get_model_outputs(wide_inputs, deep_inputs, dnn_hidden_units)\n",
    "\n",
    "    # Build model and compile it all together\n",
    "    model = tf.keras.models.Model(inputs=inputs, outputs=output)\n",
    "\n",
    "    n_intervals = len(BREAKS)-1\n",
    "\n",
    "    model.compile(loss=nnet_survival.surv_likelihood(n_intervals), \n",
    "                    metrics = nnet_survival.surv_likelihood(n_intervals),\n",
    "                    optimizer=tf.keras.optimizers.Adam(),\n",
    "                    run_eagerly=True)\n",
    "\n",
    "    return model\n",
    "\n",
    "def build_wide_deep_model_test():\n",
    "    \"\"\"Builds wide and deep model using Keras Functional API.\n",
    "\n",
    "    Returns:\n",
    "        `tf.keras.models.Model` object.\n",
    "    \"\"\"\n",
    "    n_intervals = len(BREAKS)-1\n",
    "    model = tf.keras.Sequential()\n",
    "    model.add(tf.keras.layers.Dense(64, input_shape=(10,),activation='relu'))\n",
    "    model.add(tf.keras.layers.Dense(32, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(0.25))\n",
    "\n",
    "    prop_hazards=0\n",
    "    if prop_hazards:\n",
    "        model.add(tf.keras.layers.Dense(1, use_bias=0, kernel_initializer='zeros'))\n",
    "        model.add(nnet_survival.PropHazards(n_intervals))\n",
    "    else:\n",
    "        model.add(tf.keras.layers.Dense(n_intervals, kernel_initializer='zeros', bias_initializer='zeros'))\n",
    "        model.add(tf.keras.layers.Activation('sigmoid'))\n",
    "\n",
    "    model.compile(loss=nnet_survival.surv_likelihood(n_intervals), optimizer=tf.keras.optimizers.Adam())\n",
    "    return model\n",
    "     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = build_wide_deep_model()\n",
    "print(\"Here is our Wide-and-Deep architecture so far:\\n\")\n",
    "print(model.summary())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "num_epochs = 1000\n",
    "batch_size = 32\n",
    "train_examples = 50000\n",
    "\n",
    "trainds = load_dataset(\n",
    "    input_file = \"../data/task2/train_data.csv\",\n",
    "    batch_size = batch_size,\n",
    "    mode = tf.estimator.ModeKeys.TRAIN)\n",
    "\n",
    "evalds = load_dataset(\n",
    "    input_file = \"../data/task2/test_data.csv\", \n",
    "    batch_size = 1000,\n",
    "    mode = tf.estimator.ModeKeys.EVAL)\n",
    "#if args[\"eval_steps\"]:\n",
    "#    self.evalds = self.evalds.take(count=args[\"eval_steps\"])\n",
    "\n",
    "\n",
    "num_batches = batch_size * num_epochs\n",
    "steps_per_epoch = train_examples // num_batches\n",
    "\n",
    "checkpoint_path = os.path.join(\"output\", \"checkpoints\")\n",
    "cp_callback = tf.keras.callbacks.ModelCheckpoint(\n",
    "    filepath=checkpoint_path, verbose=1, save_weights_only=True)\n",
    "\n",
    "history = model.fit(\n",
    "    trainds,\n",
    "    validation_data=evalds,\n",
    "    epochs=num_epochs,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    verbose=2,  # 0=silent, 1=progress bar, 2=one line per epoch\n",
    "    callbacks=[cp_callback])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv_2",
   "language": "python",
   "name": "venv_2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
